<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://tungdnguyen.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://tungdnguyen.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-11-19T01:05:24+00:00</updated><id>https://tungdnguyen.github.io/feed.xml</id><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Music Genre Conversion</title><link href="https://tungdnguyen.github.io/projects/2019/music-genres/" rel="alternate" type="text/html" title="Music Genre Conversion"/><published>2019-10-20T15:12:00+00:00</published><updated>2019-10-20T15:12:00+00:00</updated><id>https://tungdnguyen.github.io/projects/2019/music-genres</id><content type="html" xml:base="https://tungdnguyen.github.io/projects/2019/music-genres/"><![CDATA[<div class="row"> <div class="col-sm-6" style="font-weight:300;"> <strong> Authors:</strong> Tung Nguyen </div> </div> <div class="row"> <div class="col-sm-6" style="font-weight:300;"> <strong> Advisor:</strong> Edward M. Reingold </div> </div> <hr/> <h3> Details </h3> <p>Fast Fourier Transform has been the backbone of signal processing. Under professor Edward M. Reingold, I conducted an independent research study on Fast Fourier Transform and its application on audio processing.</p> <p>The goal of the project is to create a music genre-conversion model. Given an audio file, the model would algorithmatically modify the song to a different genre. The problem statement would then be:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Create a Folk version of Nirvana's Smells Like Teen Spirit using Deep Neural Network
</code></pre></div></div> <h4> Approach </h4> <ul> <li>Created an adversarial network including one conversion model and an adversarial music genre classifier.</li> <li>Using Short Time Fourier Transform, vectorized audio files in <a href="https://www.kaggle.com/datasets/andradaolteanu/gtzan-dataset-music-genre-classification">GTZAN Dataset</a>.</li> <li>Using vectorize audio as input, created a music genre classification model.</li> <li>Trained a music generation model for each genre using GTZAN as our training data. These models are called <code class="language-plaintext highlighter-rouge">genre blueprints</code>.</li> </ul> <h4> Results </h4> <p>Folk -&gt; Rock conversion on Rock blueprints achieved <strong>60% accuracy</strong> on genre classifier.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <audio src="/assets/audio/sample_folk_song.mp3" controls=""/> </figure> </div> </div> <div class="caption"> Original Folk song </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <audio src="/assets/audio/folk_x_average_rock_footprint.mp3" controls=""/> </figure> </div> </div> <div class="caption"> An example of Folk -&gt; Rock </div>]]></content><author><name></name></author><category term="adversarial-network"/><category term="signal-processing"/><category term="music-generation"/><category term="machine-learning"/><category term="deep-learning"/><summary type="html"><![CDATA[using Fast Fourier Transform and neural net for music genre translation]]></summary></entry><entry><title type="html">Spotify’s Social Recommender</title><link href="https://tungdnguyen.github.io/projects/2019/spotify-taste/" rel="alternate" type="text/html" title="Spotify’s Social Recommender"/><published>2019-10-20T15:12:00+00:00</published><updated>2019-10-20T15:12:00+00:00</updated><id>https://tungdnguyen.github.io/projects/2019/spotify-taste</id><content type="html" xml:base="https://tungdnguyen.github.io/projects/2019/spotify-taste/"><![CDATA[<div class="row"> <div class="col-sm-6" style="font-weight:300;"> <strong> Authors:</strong> Tung Nguyen, Tuan Tran, Paolo Ratti </div> </div> <div class="row"> <div class="col-sm-6" style="font-weight:300;"> <strong> Advisor:</strong> Aron Culotta - <a target="_blank" rel="noopener noreferrer" href="http://tapilab.github.io/"> TAPILAB </a> </div> </div> <div class="row"> <div class="col-sm-3" style="font-weight:300;"> <strong> Artifacts: </strong><a target="_blank" rel="noopener noreferrer" href="/assets/pdf/spotify.pdf"> Presentation </a> </div> </div> <hr/> <h3> Details </h3> <p>Spotify is one of the biggest music streaming services. The service provides social feature which allows an user to follow other users’ music playlists. The community can also build a shared playlist together. Using Spotify users’ playlists and their corresponding Twitter’s networks, we want to understand how friends (and followers) influence users’ music taste. Consequently, we aim to build a music recommender based on user’s follower network.</p> <h4> Approach </h4> <ul> <li>Collected Spotify users’ playlist and friends network. We scraped Twitter Streaming API for public tweets that contain #NowPLaying hashtag, which is a Spotify specific tag. We consider these users to be highly influential on their followers’ music predilection.</li> <li>Created a follower network among collected users.</li> <li>Created users’ taste profiles based on their playlists characteristics: acousticness, danceability, energy, speechiness,and valence.</li> <li>Incorporated Last.FM’s song tags to analyze users’ music listening habits (diversity, genre, etc.,)</li> <li>Built a collaborative-filtering recommender which predicts songs an user most likely listen to next based on their friends’ profiles.</li> </ul> <h4> Results </h4> <ul> <li>Achieved <strong>0.77 AUC</strong> on 6-fold cross-validation in our next-song prediction model. A <strong>40% increase</strong> from baseline Spotify’s current recommender (0.46 AUC).</li> <li>Collected ~5k Spotify users’ playlist and following/followers.</li> <li>For 1000 users with taste profiles, 14% of the songs that appeared in a follower/friend’s playlist would appear on the seed users’ playlist later on.</li> <li>41% of the friends/followers of a seed user will share at least a common song with the seed user.</li> <li>Understood users’ listening habits and playlist diversity using text tokenization, PCA, and cosine similarity.</li> <li>Observed a strong correlation between of Spotify’s current recommender and users’ listening habits.</li> </ul> <div class="row mt-6-9"> <figure> <picture> <img src="/assets/img/spotify/music_habit.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Users' listening habit over time: cosine similarity between newly created playlist with existing playlists </div> <div class="row mt-6-9"> <figure> <picture> <img src="/assets/img/spotify/spotify_recommend.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Correlation between Spotify recommendations and users' playlists (Our baseline) </div> <div class="row mt-6-9"> <figure> <picture> <img src="/assets/img/spotify/auc.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Collaborative-filtering recommender AUC on 6-fold cross validation.<br/> Our model yields <strong>0.76 AUC</strong>, a <strong>40% increase</strong> from baseline 0.46 AUC. </div> <h4> Paper </h4> <object data="/assets/pdf/spotify.pdf" width="850" height="900" type="application/pdf"></object>]]></content><author><name></name></author><category term="social-computing"/><category term="recommendation-system"/><category term="collaborative-filtering"/><category term="followers-network"/><category term="community-detection"/><category term="machine-learning"/><category term="nlp"/><category term="social-network-analysis"/><summary type="html"><![CDATA[spotify’s music recommendation based on users’ followers' network]]></summary></entry><entry><title type="html">Competitive Gaming Matches Prediction</title><link href="https://tungdnguyen.github.io/projects/2018/league-predict/" rel="alternate" type="text/html" title="Competitive Gaming Matches Prediction"/><published>2018-10-20T15:12:00+00:00</published><updated>2018-10-20T15:12:00+00:00</updated><id>https://tungdnguyen.github.io/projects/2018/league-predict</id><content type="html" xml:base="https://tungdnguyen.github.io/projects/2018/league-predict/"><![CDATA[<p align="center"><iframe width="560" height="315" src="https://www.youtube.com/embed/KlYK1G1af1s" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe> </p> <p><br/></p> <h3> Details </h3> <div class="row"> <div class="col-sm-6" style="font-weight:300;"> <strong> Authors:</strong> Tung Nguyen, James Guerrera-Sapone </div> </div> <div class="row"> <div class="col-sm-3" style="font-weight:300;"> <strong> Artifacts: <a target="_blank" rel="noopener noreferrer" href="https://github.com/tungdnguyen/league_of_legends_predict"> Code </a> | <a target="_blank" rel="noopener noreferrer" href="/assets/pdf/league_predict.pdf"> Paper </a> </strong> </div> </div> <p><br/></p> <p>League of Legends (LoL) is one of the most popular online video games in the world. A game match contains 2 teams of 5 players, and each player controls a character with a unique set of abilities. This means that there are many factors which can influence which team wins. There are influences both from the characters being used as well as the individual skill of the players. We have created a data pipeline using Microsoft Azure to process data from the LoL developer API and developed a machine learning model to predict the winning team based both on the player and character compositions of each team.</p> <p><strong>Notable work</strong></p> <ul> <li>Created a data pipeline to collect data from League of Legends Developer API to store on Azure Cosmos DB.</li> <li>Performed data processing and built a Logistic Regression model to predict outcome of each game match on Apache Spark (Azure HDInsight and Spark ML).</li> <li>Implemented the prediction model on an interactive web application.</li> </ul> <div class="row mt-3"> <figure> <picture> <img src="/assets/img/league/pipeline.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Pipeline Overview </div> <div class="row mt-3"> <figure> <picture> <img src="/assets/img/league/data_collection.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> How League of Legends data is collected </div> <h4> Paper </h4> <object data="/assets/pdf/league_predict.pdf" width="850" height="900" type="application/pdf"></object>]]></content><author><name></name></author><category term="machine-learning,"/><category term="spark,"/><category term="cosmos-db,"/><category term="azure,"/><category term="cloud"/><summary type="html"><![CDATA[utilize big data technologies to predict outcomes of League of Legends matches]]></summary></entry><entry><title type="html">Self-driving NASA Mars Miner</title><link href="https://tungdnguyen.github.io/projects/2018/self-driving-robot/" rel="alternate" type="text/html" title="Self-driving NASA Mars Miner"/><published>2018-10-20T15:12:00+00:00</published><updated>2018-10-20T15:12:00+00:00</updated><id>https://tungdnguyen.github.io/projects/2018/self-driving-robot</id><content type="html" xml:base="https://tungdnguyen.github.io/projects/2018/self-driving-robot/"><![CDATA[<h3> Details </h3> <div class="row"> <div class="col-sm-3" style="font-weight:300;"> <strong> Authors:</strong> Tung Nguyen, Scarlett Hawk IIT </div> </div> <div class="row"> <div class="col-sm-3" style="font-weight:300;"> <strong>Artifacts: <a target="_blank" rel="noopener noreferrer" href="https://github.com/tungdnguyen/nasa-robot"> Code </a> </strong> </div> </div> <hr/> <p><a href="https://www.nasa.gov/press-release/nasa-announces-robotic-mining-competition-0">LUNABOTICS</a> is an Artemis Student Challenge to educate college students on NASA Systems Engineering. Participating teams design and build a prototype robot for long-term human presence on Mars. During the simulation, robots must overcome regolith simulants, resources, weight, size, and remote/autonomous operations.</p> <div class="row mt-3"> <figure> <picture> <img src="/assets/img/robot/team.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> IIT team at Kenedy Space Center. </div> <p>I was part of Illinois Tech’s automation team. We worked closely with fellow mechanical and electrical engineers to build a self-maneuvered system for the robot.</p> <p>Our main achievements:</p> <ul> <li>Created a neural obstacle detection system using Kinect’s depth sensors, LibFreenect, and OpenCV</li> <li>Integrated LIDAR and Adafruit Capacitive touch sensors to develop a fully automated robot maneuvering system</li> <li>Designed manual control system fall-back</li> <li>Successfully ran self-driving test drives on real mining evironment.</li> </ul> <p align="center"><iframe width="560" height="315" src="https://www.youtube.com/embed/5FNd6rH5uJ4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></p> <div class="caption"> Proof of life for mining robot </div> <p><strong>Personal Note</strong>: My desire to participate in this competition stemmed from watching <a href="https://www.youtube.com/watch?v=FI2VTcHbP4Q">Vietnam Robocon</a>, a robot contest for college students across the country, during my childhood. Vietnam’s tremendous effort in promoting STEM competitions/programs on national television during the 2000s incited my passion for STEMs. I hope Vietnamese govenment and education organizations can continue their supports for these programs in the future.</p> <p align="center"><iframe width="560" height="315" src="https://www.youtube.com/embed/m5mSOAzYh6s" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></p> <div class="caption"> Vietnam Robocon team claimed champion in Asia-Pacific Robot Contest in 2004. <br/> The first of our 6 champion titles during the competion's 20 years history. </div>]]></content><author><name></name></author><category term="robotic"/><category term="computer-vision"/><category term="nasa"/><category term="mining"/><category term="kinect"/><category term="self-driving"/><summary type="html"><![CDATA[Automated control system for 2018 NASA Robotic Mining Competition]]></summary></entry><entry><title type="html">Computer Graphics Projects</title><link href="https://tungdnguyen.github.io/projects/2017/graphic/" rel="alternate" type="text/html" title="Computer Graphics Projects"/><published>2017-10-20T15:12:00+00:00</published><updated>2017-10-20T15:12:00+00:00</updated><id>https://tungdnguyen.github.io/projects/2017/graphic</id><content type="html" xml:base="https://tungdnguyen.github.io/projects/2017/graphic/"><![CDATA[<p align="center"> <iframe width="560" height="315" src="https://www.youtube.com/embed/oQcBINYvuk8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe> </p> <div class="caption">3D rendering with Phong illumination model</div> <p><br/></p> <h3> Details </h3> <div class="row"> <div class="col-sm-3" style="font-weight:300;"> <strong> Authors:</strong> Tung Nguyen </div> </div> <hr/> <p>Studied Computer Graphics in class <a href="http://www.cs.iit.edu/~agam/cs411/index.html">CS411-IIT</a>, under professor Gady Agam.<br/> <strong>Notable topics</strong>: Graphics programming with WebGL; Raster graphics; 2D modeling and viewing; Curve and surface interpolation; 3D Rendering with WebGL; 3D Modeling and viewing; Illumination models and surface rendering; Animation</p> <h3>Selected projects</h3> <ul> <li>2D modeling/viewing with WebGL</li> <li>Curves rendering with Bresenham line drawing algorithm and Cardinal splines interpolation</li> <li>3D rendering with Phong illumination model with WebGL and vectorized object files</li> </ul>]]></content><author><name></name></author><category term="computer-graphics"/><category term="webgl"/><category term="javascript"/><category term="illumination-model"/><category term="rendering"/><summary type="html"><![CDATA[rendering 3D model using various computer graphic techniques]]></summary></entry><entry><title type="html">Social Network Analysis Projects</title><link href="https://tungdnguyen.github.io/projects/2016/facebook/" rel="alternate" type="text/html" title="Social Network Analysis Projects"/><published>2016-10-20T15:12:00+00:00</published><updated>2016-10-20T15:12:00+00:00</updated><id>https://tungdnguyen.github.io/projects/2016/facebook</id><content type="html" xml:base="https://tungdnguyen.github.io/projects/2016/facebook/"><![CDATA[<h3> Details </h3> <div class="row"> <div class="col-sm-3" style="font-weight:300;"> <strong> Authors:</strong> Tung Nguyen </div> </div> <div class="row"> <div class="col-sm-3" style="font-weight:300;"> <strong> Artifacts: <a target="_blank" rel="noopener noreferrer" href="https://github.com/tungdnguyen/social-network-analysis"> Code </a> </strong> </div> </div> <hr/> <p>Studied social network analysis techniques in class <a href="https://github.com/iit-cs579/main/tree/master">CS579-IIT</a>, under professor Aron Culotta.<br/> Notable topics:</p> <ul> <li><strong>Social Network Analysis</strong>: social graph theory, community detections, diffusion, graph dynamics, link prediction</li> <li><strong>Natural Language Processing</strong>: sentimental analysis, recommendation system, social network texts classification, topic modeling</li> <li><strong>Information Retrieval</strong>: data extraction from social network (Facebook, Twitter)</li> </ul> <h3>Selected projects</h3> <ul> <li>Used Twitter API to construct a social network of Twitter accounts</li> <li>Implemented community detection and link prediction algorithms using Facebook “like” data.</li> <li>Clustered the Facebook graph into communities and recommended friends for Bill Gates.</li> <li>Built a sentimental classifier based on IMDB movie reviews</li> <li>Implemented a content-based recommendation algorithm, which used the list of genres for a movie as the content. The data come from the <a href="http://grouplens.org/datasets/movielens/">MovieLens project</a></li> </ul> <p><br/></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/social%20network/network.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/social%20network/network1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Communities detection using Girvan-Newman algorithm on collected Twitter followers network. </div>]]></content><author><name></name></author><category term="nlp"/><category term="social-network-analysis"/><category term="python"/><category term="twitter"/><category term="facebook"/><category term="social-computing"/><summary type="html"><![CDATA[a detailed study on various topic on social network analysis]]></summary></entry><entry><title type="html">Harmonizing</title><link href="https://tungdnguyen.github.io/projects/2015/harmonizing/" rel="alternate" type="text/html" title="Harmonizing"/><published>2015-10-20T15:12:00+00:00</published><updated>2015-10-20T15:12:00+00:00</updated><id>https://tungdnguyen.github.io/projects/2015/harmonizing</id><content type="html" xml:base="https://tungdnguyen.github.io/projects/2015/harmonizing/"><![CDATA[<div class="row mt-3"> <figure> <picture> <img src="/assets/img/harmonizing/harmonizing.gif" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> UI design by <a href="https://www.behance.net/phtran">Phuong Tran</a> </div> <h3> Details </h3> <div class="row"> <div class="col-sm-6" style="font-weight:300;"> <strong> Authors:</strong> Tung Nguyen, Linh Hoang </div> </div> <div class="row"> <div class="col-sm-6" style="font-weight:300;"> <strong> Artifacts: <a target="_blank" rel="noopener noreferrer" href="https://github.com/tungdnguyen/harmonizing"> Code </a> | <a target="_blank" rel="noopener noreferrer" href="https://devpost.com/software/harmonizing"> Hackrice </a> </strong> </div> </div> <hr/> <p>A crowdsourcing platform which lets users create community-based songs. Users can choose to pariticipate in a designated part in a song: vocal, backup, or instrumental. With provided guides, participants record their chosen part and upload it. Then, different harmony parts from different users will be selected randomly and merged into the final song.</p> <h3> Inspiration </h3> <p>Our inspiration came from experiences as members of our high school’s music club. As an Acapella club, without professional help, academic training or any advanced equipment, we always struggled to find the best way for our club members to harmonize together.</p> <h3> How we built it </h3> <p>We used node.js to create the server and Express as the framework. Then, we used Multer, a node.js middleware, to allow users to upload their record. Lastly, we used HTML to play the tracks and merge them together.</p> <p><br/></p> <p align="center"><iframe width="560" height="315" src="https://www.youtube.com/embed/D-2Fi_42aQo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></p> <div class="caption"> Participate in community-based songs </div>]]></content><author><name></name></author><category term="crowdsourcing"/><category term="crowdworking"/><category term="music"/><category term="signal-processing"/><category term="community-work"/><category term="social-computing"/><summary type="html"><![CDATA[a crowdsourcing platform for creating community-based music (HackRice 2016’s winner)]]></summary></entry><entry><title type="html">Patutu Trade</title><link href="https://tungdnguyen.github.io/projects/2015/patutu/" rel="alternate" type="text/html" title="Patutu Trade"/><published>2015-10-20T15:12:00+00:00</published><updated>2015-10-20T15:12:00+00:00</updated><id>https://tungdnguyen.github.io/projects/2015/patutu</id><content type="html" xml:base="https://tungdnguyen.github.io/projects/2015/patutu/"><![CDATA[<div class="row"> <div class="col-sm-6" style="font-weight:300;"> <strong> Authors:</strong> Tung Nguyen, Tuan Tran, Paolo Ratti </div> </div> <div class="row"> <div class="col-sm-3" style="font-weight:300;"> <strong> Artifacts: <a href="https://github.com/tungdnguyen/patutu_trade"> Code </a> </strong> </div> </div> <hr/> <p>A virtual stock-trading game for beginner in finance. It taught players how to do basic transactions and how to co-operate with media to predict the price of the stocks.</p> <p><strong>Features</strong></p> <ul> <li>Daily update for related news, stock data, predictions, graphs and invested money</li> <li>A fast-forward feature for day increment</li> <li>Ability to buy and sell stocks</li> <li>A mechanism to keep track of the total amount of money and stocks owned</li> <li>A prediction model for future price</li> </ul>]]></content><author><name></name></author><summary type="html"><![CDATA[a virtual stock-trading game for beginner in finance]]></summary></entry><entry><title type="html">Image and Real-time Video Style Transfer</title><link href="https://tungdnguyen.github.io/projects/2015/image-video-transform/" rel="alternate" type="text/html" title="Image and Real-time Video Style Transfer"/><published>2015-10-20T00:00:00+00:00</published><updated>2015-10-20T00:00:00+00:00</updated><id>https://tungdnguyen.github.io/projects/2015/image-video-transform</id><content type="html" xml:base="https://tungdnguyen.github.io/projects/2015/image-video-transform/"><![CDATA[<h3> Details </h3> <div class="row"> <div class="col-sm-6" style="font-weight:300;"> <strong> Authors:</strong> Tung Nguyen, Tuan Tran </div> </div> <div class="row"> <div class="col-sm-3" style="font-weight:300;"> <strong>Artifacts: <a target="_blank" rel="noopener noreferrer" href="https://github.com/tungdnguyen/style-transfer"> Code </a> | <a target="_blank" rel="noopener noreferrer" href="/assets/pdf/style_transfer.pdf"> Paper </a> </strong> </div> </div> <hr/> <p>The term “style transfer” has been coined to describe the problem of training machines to “roughly” mimic human’s ability to interplay different image styles to create unique and complex visual experiences. To put it simply, style transfer is the problem of recomposing images in the style of other images; that is, given an original image, and a second image with specific style, we would like to create a new image with the style of the second image applied to the content of the original image. Even though it does not fully explain how humans create unique art pieces, style transfer is definitely the right logical step towards solving the more complex problem of understanding how humans create and perceive arts.</p> <p>We used the algorithm introduced by Gatys et al. as the basis of this project. With a pre-trained VGG model and a clever loss setup, Gatys et al. algorithm is able to achieve high-quality results and also provides insights into image representations learned by Convolutional Neural Networks and empirically demonstrate CNNs’ potential for high level image synthesis and manipulation [1]. We also implemented an extension of this method proposed by Johnson et al. in order to speed up styling process. This extension allows for fast real time style transfer.</p> <div class="row mt-3"> <figure> <picture> <img src="/assets/img/transfer/Results.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="row mt-3"> <figure> <picture> <img src="/assets/img/transfer/Results_2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Results images </div> <p>To obtain a style representation from the style input image, we constructed a Gram matrix which can be built on top of the feature map in any layer of VGG16. The Gram matrix consists of the correlations between different filter responses, and thus captures the texture information of an image.</p> <p>The basic intuition behind Gram matrix is that by multiplying elements of each combination of feature maps together, we are essentially checking if the elements “overlap” at their location in the image. The summation then discards all the information’s spatial relevance.</p> <div class="row mt-3"> <figure> <picture> <img src="/assets/img/transfer/training_1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="row mt-3"> <figure> <picture> <img src="/assets/img/transfer/training_2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Training </div> <h4> Paper </h4> <object data="/assets/pdf/style_transfer.pdf" width="850" height="900" type="application/pdf"></object>]]></content><author><name></name></author><category term="style-transfer"/><category term="computer-vision"/><summary type="html"><![CDATA[image and video style transferring using deep neural net]]></summary></entry></feed>