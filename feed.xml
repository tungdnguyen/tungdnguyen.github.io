<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://tungdnguyen.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://tungdnguyen.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2023-08-14T17:22:26+00:00</updated><id>https://tungdnguyen.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">Music genre conversion with Fast Fourier Transform</title><link href="https://tungdnguyen.github.io/projects/2019/music-genres/" rel="alternate" type="text/html" title="Music genre conversion with Fast Fourier Transform"/><published>2019-10-20T15:12:00+00:00</published><updated>2019-10-20T15:12:00+00:00</updated><id>https://tungdnguyen.github.io/projects/2019/music-genres</id><content type="html" xml:base="https://tungdnguyen.github.io/projects/2019/music-genres/"><![CDATA[<div class="row"> <div class="col-sm-6" style="font-weight:300;"> <strong> Authors:</strong> Tung Nguyen </div> </div> <div class="row"> <div class="col-sm-6" style="font-weight:300;"> <strong> Advisor:</strong> Edward M. Reingold </div> </div> <hr/> <h3> Details </h3> <p>Fast Fourier Transform has been the backbone of signal processing. Under professor Edward M. Reingold, I conducted a independent research study on Fast Fourier Transform and its application on audio processing.</p> <p>The goal of the project is to create a music genre-conversion model. Given an audio file, the model would algorithmatically modify the song to a different genre. The problem statement would then be:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Create a Folk version of Nirvana's Smells Like Teen Spirit using Deep Neural Network
</code></pre></div></div> <h4> Approach </h4> <ul> <li>The idea is to create an adversarial network in which we have one conversion model, and an adversarial music genre classifier.</li> <li>Using Short Time Fourier Transform, vectorized audio files in <a href="https://www.kaggle.com/datasets/andradaolteanu/gtzan-dataset-music-genre-classification">GTZAN Dataset</a>.</li> <li>Using vectorize audio as input, created a music genre classification model.</li> <li>Train a music generation model for each genre. We use GTZAN as our training data. We call these model <code class="language-plaintext highlighter-rouge">genre blueprints</code>.</li> </ul> <h4> Results </h4> <p>Folk -&gt; Rock conversion on Rock blueprints achieve <strong>60% accuracy</strong> on genre classifier.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <audio src="/assets/audio/sample_folk_song.mp3" controls=""/> </figure> </div> </div> <div class="caption"> Original Folk song </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <audio src="/assets/audio/folk_x_average_rock_footprint.mp3" controls=""/> </figure> </div> </div> <div class="caption"> An example of Folk -&gt; Rock </div>]]></content><author><name></name></author><category term="adversarial-network"/><category term="signal-processing"/><category term="music-generation"/><category term="machine-learning"/><category term="deep-learning"/><summary type="html"><![CDATA[Authors: Tung Nguyen Advisor: Edward M. Reingold]]></summary></entry><entry><title type="html">Spotify’s social recommender</title><link href="https://tungdnguyen.github.io/projects/2019/spotify-taste/" rel="alternate" type="text/html" title="Spotify’s social recommender"/><published>2019-10-20T15:12:00+00:00</published><updated>2019-10-20T15:12:00+00:00</updated><id>https://tungdnguyen.github.io/projects/2019/spotify-taste</id><content type="html" xml:base="https://tungdnguyen.github.io/projects/2019/spotify-taste/"><![CDATA[<div class="row"> <div class="col-sm-6" style="font-weight:300;"> <strong> Authors:</strong> Tung Nguyen, Tuan Tran, Paolo Ratti </div> </div> <div class="row"> <div class="col-sm-6" style="font-weight:300;"> <strong> Advisor:</strong> Aron Culotta - <a href="http://tapilab.github.io/"> TAPILAB </a> </div> </div> <div class="row"> <div class="col-sm-3" style="font-weight:300;"> <strong> Artifacts: </strong><a href="/assets/pdf/spotify.pdf"> Presentation </a> </div> </div> <hr/> <h3> Details </h3> <p>Spotify is one of the biggest music streaming service. The service provides social feature which allow an user to follow other users’ music playlist. The community can also build a shared playlist together. Using Spotify users’ playlists and their corresponding Twitter’s networks, we want to understand how friends (and followers) influence users’ music taste. Consequently, we aim to build a music recommender based on user’s follower network.</p> <h4> Approach </h4> <ul> <li>Collected Spotify user’s playlist and friends network. We scraped Twitter Streaming API for public tweets contains #NowPLaying hashtag, which is a Spotify specific tag. We consider these users to be highly influential on their followers’ music predilection.</li> <li>Created a follower network between collected users.</li> <li>Created users’ taste profiles based on their playlists characteristics: acousticness, danceability, energy, speechiness,valence, acousticness.</li> <li>Incorporated Last.FM’s song tags to analyze users’s music listening habits (diversity, genre, etc.,)</li> <li>Built a collaborative-filtering recommender which predict songs an user most likely listen to next based on their friends’ profiles.</li> </ul> <h4> Results </h4> <ul> <li>Achieved <strong>0.77 AUC</strong> on 6-fold cross-validation in our next-song prediction model. A <strong>40% increase</strong> from baseline Spotify’s current recommender (0.46 AUC).</li> <li>Collected ~5k Spotify users’ playlist and following/followers.</li> <li>For 1000 users with taste profiles, 14% of the songs appeared in a follower/friend playlist will appeared on the seed users’playlist later on.</li> <li>41% of the friends/followers for a seed users will share at least a common song with the seed user.</li> <li>Understand user listening habits and playlist diversity using text tokenization, PCA and cosine similarity</li> <li>See strong correlation between of Spotify’s current recommender on user listening habits.</li> </ul> <div class="row mt-6-9"> <figure> <picture> <img src="/assets/img/spotify/music_habit.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> User listening habit over time: cosine similarity between newly created playlist with existing playlists </div> <div class="row mt-6-9"> <figure> <picture> <img src="/assets/img/spotify/spotify_recommend.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Correlation between Spotify recommendations and user playlist (Our baseline) </div> <div class="row mt-6-9"> <figure> <picture> <img src="/assets/img/spotify/auc.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Collaborative-filtering recommender AUC on 6-fold cross validation.<br/> Our model yields <strong>0.76 AUC</strong>, a <strong>40% increase</strong> from baseline 0.46 AUC. </div>]]></content><author><name></name></author><category term="social-computing"/><category term="recommendation-system"/><category term="collaborative-filtering"/><category term="followers-network"/><category term="community-detection"/><category term="machine-learning"/><category term="nlp"/><category term="social-network-analysis"/><summary type="html"><![CDATA[Spotify’s music recommendation based on users’ followers' network]]></summary></entry><entry><title type="html">Competitive gaming matches prediction</title><link href="https://tungdnguyen.github.io/projects/2018/league-predict/" rel="alternate" type="text/html" title="Competitive gaming matches prediction"/><published>2018-10-20T15:12:00+00:00</published><updated>2018-10-20T15:12:00+00:00</updated><id>https://tungdnguyen.github.io/projects/2018/league-predict</id><content type="html" xml:base="https://tungdnguyen.github.io/projects/2018/league-predict/"><![CDATA[<p align="center"><iframe width="560" height="315" src="https://www.youtube.com/embed/KlYK1G1af1s" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe> </p> <p><br/></p> <h3> Details </h3> <div class="row"> <div class="col-sm-6" style="font-weight:300;"> <strong> Authors:</strong> Tung Nguyen, James Guerrera-Sapone </div> </div> <div class="row"> <div class="col-sm-3" style="font-weight:300;"> <strong> Artifacts: <a href="https://github.com/tungdnguyen/league_of_legends_predict"> Code </a> | <a href="/assets/pdf/league_predict.pdf"> Paper </a> </strong> </div> </div> <hr/> <p>League of Legends (LoL) is one of the most popular online video games in the world. A game match contains 2 teams of 5 players and each player controls a character with a unique set of abilities. This means that there are many factors which can influence which team wins. There are influences both from the characters being used as well as the individual skill of the players. We have created a data pipeline using Microsoft Azure to process data from the LoL developer API and created a machine learning model to predict the winning team based both off of the player and character compositions of each team.</p> <p><strong>Notable work</strong></p> <ul> <li>Created a data pipeline to collect data from League of Legends Developer API to store on Azure Cosmos DB.</li> <li>Performed data processing and built a Logistic Regression model to predict outcome of each game match on Apache Spark (Azure HDInsight and Spark ML).</li> <li>Implemented the prediction model on an interactive web application.</li> </ul> <div class="row mt-3"> <figure> <picture> <img src="/assets/img/league/pipeline.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Pipeline Overview </div> <div class="row mt-3"> <figure> <picture> <img src="/assets/img/league/data_collection.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> How League of Legends data is collected </div>]]></content><author><name></name></author><category term="machine-learning,"/><category term="spark,"/><category term="cosmos-db,"/><category term="azure,"/><category term="cloud"/><summary type="html"><![CDATA[Utilize big data technologies to predict outcomes of League of Legends matches]]></summary></entry><entry><title type="html">Self-driving Mars Robotic Miner in NASA Robotic Mining Competition 2018</title><link href="https://tungdnguyen.github.io/projects/2018/self-driving-robot/" rel="alternate" type="text/html" title="Self-driving Mars Robotic Miner in NASA Robotic Mining Competition 2018"/><published>2018-10-20T15:12:00+00:00</published><updated>2018-10-20T15:12:00+00:00</updated><id>https://tungdnguyen.github.io/projects/2018/self-driving-robot</id><content type="html" xml:base="https://tungdnguyen.github.io/projects/2018/self-driving-robot/"><![CDATA[<h3> Details </h3> <div class="row"> <div class="col-sm-3" style="font-weight:300;"> <strong> Authors:</strong> Tung Nguyen, Scarlett Hawk IIT </div> </div> <div class="row"> <div class="col-sm-3" style="font-weight:300;"> <strong>Artifacts: <a href="https://github.com/tungdnguyen/nasa-robot"> Code </a> </strong> </div> </div> <hr/> <p><a href="https://www.nasa.gov/press-release/nasa-announces-robotic-mining-competition-0">LUNABOTICS</a> is an Artemis Student Challenge to educate college students on NASA Systems Engineering. Participating teams design and build a prototype robot for long-term human presence on Mars. During the simulation, robots must overcome regolith simulants, resources, weight, size, and remote/autonomous operations.</p> <div class="row mt-3"> <figure> <picture> <img src="/assets/img/robot/team.jpeg" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> IIT team at Kenedy Space Center. </div> <p>I was part of Illinois Tech’s automation team. We worked closely with fellow mechanical and electrical engineers to build a self-maneuvered system for the robot.</p> <p>Our main achievements:</p> <ul> <li>Created a neural obstacle detection system using Kinect’s depth sensors and LibFreenect and OpenCV</li> <li>Integrated LIDAR and Adafruit Capacitive touch sensors to develop a fully automated robot maneuvering system</li> <li>Designd manual control system fall-back</li> <li>Successfully ran self-driving test drives on real mining evironment.</li> </ul> <p align="center"><iframe width="560" height="315" src="https://www.youtube.com/embed/5FNd6rH5uJ4" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></p> <div class="caption"> Proof of life for mining robot </div> <p><strong>Personal Note</strong>: My desire to participate in this competition was inspired by watching <a href="https://www.youtube.com/watch?v=FI2VTcHbP4Q">Vietnam Robocon</a>,a robot contest for college students across the country, during my childhood. Vietnam’s tremendous effort on promoting STEM competitions/programs on national television during the 2000s incite my passion for STEMs. I hope Vietnamese govenment and education organizations can continue their supports for these programs during the future.</p> <p align="center"><iframe width="560" height="315" src="https://www.youtube.com/embed/m5mSOAzYh6s" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></p> <div class="caption"> Vietnam Robocon team claimed champion in Asia-Pacific Robot Contest in 2004. <br/> The first of our 6 champion titles during the competion's 20 years history. </div>]]></content><author><name></name></author><category term="robotic"/><category term="computer-vision"/><category term="nasa"/><category term="mining"/><category term="kinect"/><category term="self-driving"/><summary type="html"><![CDATA[Build a mining robot for NASA Mars missions]]></summary></entry><entry><title type="html">Studies on Computer Graphics</title><link href="https://tungdnguyen.github.io/projects/2017/graphic/" rel="alternate" type="text/html" title="Studies on Computer Graphics"/><published>2017-10-20T15:12:00+00:00</published><updated>2017-10-20T15:12:00+00:00</updated><id>https://tungdnguyen.github.io/projects/2017/graphic</id><content type="html" xml:base="https://tungdnguyen.github.io/projects/2017/graphic/"><![CDATA[<p align="center"> <iframe width="560" height="315" src="https://www.youtube.com/embed/oQcBINYvuk8" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe> </p> <div class="caption">3D rendering with Phong illumination model</div> <p><br/></p> <h3> Details </h3> <div class="row"> <div class="col-sm-3" style="font-weight:300;"> <strong> Authors:</strong> Tung Nguyen </div> </div> <hr/> <p>Studied Computer Graphics in class <a href="http://www.cs.iit.edu/~agam/cs411/index.html">CS411-IIT</a>, under professor Gady Agam.<br/> <strong>Notable topics</strong>: graphics programming with WebGL; Raster graphics; 2D modeling and viewing; Curve and surface interpolation; 3D Rendering with WebGL; 3D Modeling and viewing; Illumination models and surface rendering; Animation</p> <h3>Selected projects</h3> <ul> <li>2D modeling/viewing with WebGL</li> <li>Curves rendering with Bresenham line drawing algorithm and Cardinal splines interpolation</li> <li>3D rendering with Phong illumination model with WebGL and vectorized object files</li> </ul>]]></content><author><name></name></author><category term="computer-graphics"/><category term="webgl"/><category term="javascript"/><category term="illumination-model"/><category term="rendering"/><summary type="html"><![CDATA[Rendering 3D model using various computer graphic techniques]]></summary></entry><entry><title type="html">Studies on Social Network Analysis</title><link href="https://tungdnguyen.github.io/projects/2016/facebook/" rel="alternate" type="text/html" title="Studies on Social Network Analysis"/><published>2016-10-20T15:12:00+00:00</published><updated>2016-10-20T15:12:00+00:00</updated><id>https://tungdnguyen.github.io/projects/2016/facebook</id><content type="html" xml:base="https://tungdnguyen.github.io/projects/2016/facebook/"><![CDATA[<h3> Details </h3> <div class="row"> <div class="col-sm-3" style="font-weight:300;"> <strong> Authors:</strong> Tung Nguyen </div> </div> <div class="row"> <div class="col-sm-3" style="font-weight:300;"> <strong> Artifacts: <a href="https://github.com/tungdnguyen/social-network-analysis"> Code </a> </strong> </div> </div> <hr/> <p>Studied social network analysis techniques in class <a href="https://github.com/iit-cs579/main/tree/master">CS579-IIT</a>, under professor Aron Culotta.<br/> Notable topics:</p> <ul> <li><strong>Social Network Analysis</strong>: social graph theory, community detections, diffusion, graph dynamics, link prediction</li> <li><strong>Natural Language Processing</strong>: sentimental analysis, recommendation system, social network texts classification, topic modeling</li> <li><strong>Information Retrieval</strong>: extracting data from social network (facebook, twitter)</li> </ul> <h3>Selected projects</h3> <ul> <li>Use Twitter API to construct a social network of Twitter accounts</li> <li>Implement community detection and link prediction algorithms using Facebook “like” data.</li> <li>Cluster the Facebook graph into communities, and recommend friends for Bill Gates.</li> <li>Sentimental classifier based on IMDB movie reviews</li> <li>Implement a content-based recommendation algorithm. It will use the list of genres for a movie as the content. The data come from the <a href="http://grouplens.org/datasets/movielens/">MovieLens project</a></li> </ul> <p><br/></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/social%20network/network.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <img src="/assets/img/social%20network/network1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Communities detection using Girvan-Newman algorithm on collected Twitter followers network. </div>]]></content><author><name></name></author><category term="nlp"/><category term="social-network-analysis"/><category term="python"/><category term="twitter"/><category term="facebook"/><category term="social-computing"/><summary type="html"><![CDATA[A detailed study on various topic on social network analysis]]></summary></entry><entry><title type="html">Harmonizing - A crowdsourcing music making platform</title><link href="https://tungdnguyen.github.io/projects/2015/harmonizing/" rel="alternate" type="text/html" title="Harmonizing - A crowdsourcing music making platform"/><published>2015-10-20T15:12:00+00:00</published><updated>2015-10-20T15:12:00+00:00</updated><id>https://tungdnguyen.github.io/projects/2015/harmonizing</id><content type="html" xml:base="https://tungdnguyen.github.io/projects/2015/harmonizing/"><![CDATA[<div class="row mt-3"> <figure> <picture> <img src="/assets/img/harmonizing/harmonizing.gif" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> UI design by <a href="https://www.behance.net/phtran">Phuong Tran</a> </div> <h3> Details </h3> <div class="row"> <div class="col-sm-3" style="font-weight:300;"> <strong> Authors:</strong> Tung Nguyen, Linh Hoang </div> </div> <div class="row"> <div class="col-sm-3" style="font-weight:300;"> <strong> Artifacts: <a href="https://github.com/tungdnguyen/harmonizing"> Code </a> | <a href="https://devpost.com/software/harmonizing"> Hackrice </a> </strong> </div> </div> <hr/> <p>A crowdsourcing platform which let users creating community-based songs. Users can choose to pariticipate in a designated part in a song: vocal, backup, instrumental. With provided guides, participants record their chosen part and upload it. Then, different harmony parts from different users will be selected randomly and merged into the final song.</p> <h3> Inspiration </h3> <p>As members of our high school’s music club. Started as an Acapella club, without professional help, academic training or any advanced equipment, we always struggled to find the best way for our club members to harmonize together.</p> <h3> How we built it </h3> <p>We use node.js to create the server and Express as the framework. Then, we use Multer, a node.js middleware to allow users to upload their record. Lastly, we use HTML to play the tracks and merge them together.</p> <p><br/></p> <p align="center"><iframe width="560" height="315" src="https://www.youtube.com/embed/D-2Fi_42aQo" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></p> <div class="caption"> Participate in community-based songs </div>]]></content><author><name></name></author><category term="crowdsourcing"/><category term="crowdworking"/><category term="music"/><category term="signal-processing"/><category term="community-work"/><category term="social-computing"/><summary type="html"><![CDATA[A crowdsourcing platform for creating community-based music (HackRice 2016’s winner)]]></summary></entry><entry><title type="html">Patutu Trade</title><link href="https://tungdnguyen.github.io/projects/2015/patutu/" rel="alternate" type="text/html" title="Patutu Trade"/><published>2015-10-20T15:12:00+00:00</published><updated>2015-10-20T15:12:00+00:00</updated><id>https://tungdnguyen.github.io/projects/2015/patutu</id><content type="html" xml:base="https://tungdnguyen.github.io/projects/2015/patutu/"><![CDATA[<div class="row"> <div class="col-sm-6" style="font-weight:300;"> <strong> Authors:</strong> Tung Nguyen, Tuan Tran, Paolo Ratti </div> </div> <div class="row"> <div class="col-sm-3" style="font-weight:300;"> <strong> Artifacts: <a href="https://github.com/tungdnguyen/patutu_trade"> Code </a> </strong> </div> </div> <hr/> <p>A virtual stock-trading game for beginner in finance. It taught players how to do basic transactions and how to co-operate with media to predict the price of the stocks.</p> <p><strong>Features</strong></p> <ul> <li>Related news, stock data, predictions, graphs and invested money are updated on a daily basis</li> <li>A fast-forward feature for day increment</li> <li>Allows for buying and selling stocks</li> <li>A mechanism to keep track of the total amount of money and stocks owned</li> <li>A prediction model for future price</li> </ul>]]></content><author><name></name></author><summary type="html"><![CDATA[A virtual stock-trading game for beginner in finance]]></summary></entry><entry><title type="html">Image and Real-time Video Style Transfer</title><link href="https://tungdnguyen.github.io/projects/2015/image-video-transform/" rel="alternate" type="text/html" title="Image and Real-time Video Style Transfer"/><published>2015-10-20T00:00:00+00:00</published><updated>2015-10-20T00:00:00+00:00</updated><id>https://tungdnguyen.github.io/projects/2015/image-video-transform</id><content type="html" xml:base="https://tungdnguyen.github.io/projects/2015/image-video-transform/"><![CDATA[<h3> Details </h3> <p>div class=”row” &gt; &lt;div class="col-sm-3" style="font-weight:300;"&gt; <strong> Authors:</strong> Tung Nguyen, Tuan Tran &lt;/div&gt; &lt;/div&gt;</p> <div class="row"> <div class="col-sm-3" style="font-weight:300;"> <strong>Artifacts: <a href="https://github.com/tungdnguyen/style-transfer"> Code </a> | <a href="/assets/pdf/style_transfer.pdf"> Paper </a> </strong> </div> </div> <hr/> <p>The term “style transfer” has been coined to describe such problem of training machines to “roughly” mimic human’s ability to interplay different image styles to create unique and complex visual experiences. To put it simply, style transfer is the problem of recomposing images in the style of other images; that is, given an original image, and a second image with specific style, we would like to create a new image with the style of the second image applied to the content of the original image. Even though it does not fully explain how humans create unique art pieces, style transfer is definitely the right logical step towards solving the more complex problem of understanding how humans create and perceive arts.</p> <p>We’ll use the algorithm introduced by Gatys et al. as the basis of this project. With a pre-trained VGG model and a clever loss setup, Gatys et al. algorithm is able to achieve high-quality results and also provides insights into image representations learned by Convolutional Neural Networks and empirically demonstrate CNNs’ potential for high level image synthesis and manipulation [1]. We’ll also implement an extension of this method proposed by Johnson et al. in order to speed up styling process. This extension allows for fast real time style transfer.</p> <div class="row mt-3"> <figure> <picture> <img src="/assets/img/transfer/Results.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="row mt-3"> <figure> <picture> <img src="/assets/img/transfer/Results_2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Results images </div> <p>To obtain a style representation from the style input image, we construct a Gram matrix which can be built on top of the feature map in any layer of VGG16. The Gram matrix consists of the correlations between different filter responses, and thus captures the texture information of an image.</p> <p>The basic intuition behind Gram matrix is that by multiplying elements of each combination of feature maps together, we are essentially checking if the elements “overlap” at their location in the image. The summation then discards all the information’s spatial relevance.</p> <div class="row mt-3"> <figure> <picture> <img src="/assets/img/transfer/training_1.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="row mt-3"> <figure> <picture> <img src="/assets/img/transfer/training_2.png" class="img-fluid rounded z-depth-1" width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="caption"> Training </div>]]></content><author><name></name></author><summary type="html"><![CDATA[Image and video style transferring using Deep Neural Network]]></summary></entry></feed>